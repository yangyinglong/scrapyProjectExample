安装Scrapy:
	sudo pip install Scrapy

查看版本:
	scrapy -h

进入保存项目文件目录:
	cd ..

创建项目:name
	scrapy startproject <project name>

cd <project name>
	scrapy.cfg				#设置项目配置
	<project name>/	
		__init__.py  
		middlewares.py  	
		settings.py			#该文件定义了一些设置，如用户代理、爬取延时等
		items.py            #该文件定义了待抓取域的模型  
		pipelines.py        #该文件处理要抓取的域
		spiders/			#该目录存储实际的爬虫代码
			__init__.py

编写 items.py 文件：
	在类中定义国家名称和人口数量
	name = scrapy.Field()
	population = scrapy.Field()

编写爬虫:
	$ scrapy genspider country example.webscraping.com --template=crawl
	用scrapy genspider 命令，传入爬虫名、域名、以及可选的模块参数，使用内置的crawl模块创建country
	然后在spiders目录下，自动生成 country.py 文件如下：

	# -*- coding: utf-8 -*-
	import scrapy
	from scrapy.linkextractors import LinkExtractor
	from scrapy.spiders import CrawlSpider, Rule

	from example.items import ExampleItem


	class CountrySpider(CrawlSpider):
	    name = 'country'   #该属性为定义爬虫名称的字符串
	    allowed_domains = ['example.webscraping.com']  #该属性定义了可爬取的域名列表，如果没有定义该属性，则表示可以爬取任何域名
	    start_urls = ['http://example.webscraping.com/']     #该属性定义了爬虫起始URL列表。

	    rules = (
	        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),
	    )    #该属性为一个正则表达式集合，用于告知爬虫需要跟踪哪些链接
	    	 #callback函数用于解析下载得到的相应

	    #parse_item()函数提供了一个从相应中获取数据的例子CON

	    def parse_item(self, response):
	        i = ExampleItem()
	        #i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()
	        #i['name'] = response.xpath('//div[@id="name"]').extract()
	        #i['description'] = response.xpath('//div[@id="description"]').extract()
	        return i

优化设置:
	默认情况下，Scrapy对同一域名允许最多8个并发下载，并且两次下载之间没有延时，这样很容易被查封
	所以在settings.py中添加如下，使爬虫同时最多只能对每个域名发起一个请求，并且两次请求之间存在延时
	CONCURRENT_REQUESTS_PER_DOMAIN = 1
	DOWNLOAD_DELAY = 5

测试爬虫:
	$ scrapy crawl country -s LOG_LEVEL=ERROR / DEBUG
	使用crawl命令加爬虫名称来运行爬虫， LOG_LEVEL 是设置日志级别 ERROR显示错误信息， DEBUG显示所有信息


使用shell命令抓取:
	scrapy shell http://example.webscraping.com/places/default/view/Afghanistan-1
	可以进行交互来抓取信息
	如：
		response.url
		response.status
		response.text
		response.css('').exreact()

在country.py文件中编写parse_item()方法:
	def parse_item(self, response):
        item = ExampleItem()
        name_css = 'tr#places_country__row td.w2p_fw::text'
        item['name'] = response.css(name_css).extract()
        pop_css = 'tr#places_population_row td.w2p_fw::text'
        item['population'] = response.css(pop_css).extract()
        #i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()
        #i['name'] = response.xpath('//div[@id="name"]').extract()
        #i['description'] = response.xpath('//div[@id="description"]').extract()
        return item

检查结果:
	$ scrapy crawl country --output=countries.csv -s LOG_LEVEL=INFO
	可以在parse_item()方法中添加保存结果的代码，
	也可以在--output选项中添加csv,json,xml格式的文件来保存结果

中断和恢复爬虫:
	$ scrapy crawl country -s LOG_LEVEL=DEBUG -s JOBDIR=crawls/country
	按下ctrl + c 之后，爬虫又完成了几个条目的处理之后才终止，这样才能保存爬虫状态，
	不能再次按下ctrl + c强行终止，
	之后输入同样的命令继续从暂停的地方恢复运行



